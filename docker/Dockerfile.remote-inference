# syntax=docker/dockerfile:1
# SPDX-License-Identifier: MIT
# SPDX-FileCopyrightText: 2025 nop
#
# VLA Inference Dockerfile for Remote GPU Services using rosbridge
# Build context: project root (crane_x7_vla/)

ARG CUDA_VERSION=12.9.1

# Base Stage: CUDA + Python + Tailscale
FROM nvidia/cuda:${CUDA_VERSION}-cudnn-runtime-ubuntu22.04 AS base

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    CUDA_HOME=/usr/local/cuda

# Install system dependencies and Tailscale
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates \
    curl \
    gnupg \
    lsb-release \
    python3-pip \
    python3-dev \
    libgomp1 \
    libglib2.0-0 \
    libgl1 \
    iproute2 \
    iputils-ping \
    socat \
    && curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/jammy.noarmor.gpg \
       | tee /usr/share/keyrings/tailscale-archive-keyring.gpg >/dev/null \
    && curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/jammy.tailscale-keyring.list \
       | tee /etc/apt/sources.list.d/tailscale.list \
    && apt-get update && apt-get install -y --no-install-recommends tailscale

# Upgrade pip and install compatible setuptools version
RUN --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    python3 -m pip install --upgrade pip "setuptools<78" wheel "packaging<24"

WORKDIR /workspace

# Install PyTorch with CUDA support
RUN --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    pip3 install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 \
    --index-url https://download.pytorch.org/whl/cu124

# Install VLA inference dependencies
COPY ros2/requirements.txt /tmp/requirements.txt
RUN --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    pip3 install \
    roslibpy \
    transformers \
    accelerate \
    "timm>=0.9.10,<1.0.0" \
    peft \
    huggingface_hub \
    pillow \
    numpy \
    sentencepiece \
    tokenizers

# Copy OpenVLA/prismatic source for model loading
COPY vla/src/openvla /workspace/vla/src/openvla

# Copy VLA inference scripts (crane_x7_vla package)
COPY ros2/src/crane_x7_vla/crane_x7_vla /workspace/crane_x7_vla
COPY ros2/src/crane_x7_vla/scripts /workspace/scripts

# Copy configuration and entrypoint
COPY docker/entrypoint-remote-inference.sh /entrypoint.sh
COPY docker/wait-for-peer.sh /usr/local/bin/wait-for-peer.sh
RUN chmod +x /entrypoint.sh /usr/local/bin/wait-for-peer.sh

# Create directories for model checkpoints and Tailscale state
RUN mkdir -p /workspace/models /var/lib/tailscale

# Environment variables
ENV PYTHONPATH="/workspace/vla/src/openvla:/workspace"

# CUDA/PyTorch optimization
ENV CUDA_LAUNCH_BLOCKING=0 \
    PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512" \
    TRANSFORMERS_NO_ADVISORY_WARNINGS=1 \
    TF_CPP_MIN_LOG_LEVEL=2

# Tailscale configuration (override at runtime)
ENV TS_AUTHKEY="" \
    TS_HOSTNAME="crane-x7-inference" \
    TS_STATE_DIR="/var/lib/tailscale" \
    TS_USERSPACE="true"

# rosbridge connection (override at runtime)
# Uses Tailscale MagicDNS hostname
ENV ROSBRIDGE_HOST="crane-x7-local" \
    ROSBRIDGE_PORT="9090"

# VLA configuration (override at runtime)
ENV VLA_MODEL_PATH="" \
    VLA_TASK_INSTRUCTION="pick up the object" \
    VLA_DEVICE="cuda" \
    VLA_INFERENCE_RATE="10.0" \
    VLA_UNNORM_KEY="crane_x7" \
    HF_TOKEN="" \
    HF_HOME="/root/.cache/huggingface"

# Entrypoint and default command
ENTRYPOINT ["/entrypoint.sh"]
CMD ["python3", "/workspace/scripts/vla_inference_rosbridge.py"]
