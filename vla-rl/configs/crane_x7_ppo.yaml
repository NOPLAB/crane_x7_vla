# CRANE-X7 VLA-RL Production Configuration
# Optimized for training on GPU cluster

experiment_name: crane_x7_vla_rl_prod
seed: 42
device: cuda

# Model settings
pretrained_checkpoint: openvla/openvla-7b
# Uncomment to load from SFT checkpoint
# sft_checkpoint: /workspace/vla/outputs/crane_x7_openvla/checkpoint_best

# LoRA settings (for RL fine-tuning)
lora_rank: 32
lora_alpha: 16

# Training
num_updates: 5000
language_instruction: "pick up the object and place it in the target area"

# Logging
use_wandb: true
wandb_project: crane-x7-vla-rl
log_interval: 10
eval_interval: 100
save_interval: 500
num_eval_episodes: 20

# Rollout configuration
rollout:
  simulator: maniskill
  env_id: PickPlace-CRANE-X7
  backend: gpu
  render_mode: rgb_array
  num_parallel_envs: 8
  num_rollouts_per_update: 16
  max_steps: 200
  use_binary_reward: true
  temperature: 0.8

# PPO configuration (following SimpleVLA-RL paper)
ppo:
  learning_rate: 5.0e-6
  gamma: 0.99
  gae_lambda: 0.95
  clip_ratio: 0.2
  value_loss_coef: 0.5
  entropy_coef: 0.005  # Lower entropy for more exploitation
  num_epochs: 10
  minibatch_size: 64
  max_grad_norm: 0.5
  target_kl: 0.015
  clip_value_loss: true
  value_clip_range: 0.2
